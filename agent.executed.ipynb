{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5030a0",
   "metadata": {},
   "source": [
    "# Multi-Agent Academic Paper Assistant\n",
    "\n",
    "This notebook implements a small multi-agent system for research, writing, editing, and plagiarism estimation. It demonstrates:\n",
    "\n",
    "- Multi-agent orchestration\n",
    "- Integrations (arXiv search via the `arxiv` package; optional Gemini/Google Generative AI if API key provided)\n",
    "- Session/memory persistence (JSON store)\n",
    "- Simple context compaction and observability (logging + timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e44efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:00:22.121531Z",
     "iopub.status.busy": "2025-11-15T14:00:22.121237Z",
     "iopub.status.idle": "2025-11-15T14:00:22.129405Z",
     "shell.execute_reply": "2025-11-15T14:00:22.127199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies (run once). Comment out after first run\n",
    "# !pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0e05c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:00:22.134286Z",
     "iopub.status.busy": "2025-11-15T14:00:22.134032Z",
     "iopub.status.idle": "2025-11-15T14:00:24.000837Z",
     "shell.execute_reply": "2025-11-15T14:00:23.999670Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import difflib\n",
    "\n",
    "# Optional: Google Gemini GenAI (used if GEMINI_API_KEY is set)\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    _HAS_GENAI = True\n",
    "except Exception:\n",
    "    genai = None\n",
    "    _HAS_GENAI = False\n",
    "\n",
    "import arxiv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger('paper_agent')\n",
    "\n",
    "GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')\n",
    "if _HAS_GENAI and GEMINI_API_KEY:\n",
    "    try:\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "        logger.info('Configured google.generativeai')\n",
    "    except Exception as e:\n",
    "        logger.warning('Failed to configure google.generativeai: %s', e)\n",
    "        _HAS_GENAI = False\n",
    "\n",
    "SESSION_DIR = 'sessions'\n",
    "os.makedirs(SESSION_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b92e13c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:00:24.003943Z",
     "iopub.status.busy": "2025-11-15T14:00:24.003606Z",
     "iopub.status.idle": "2025-11-15T14:00:24.011794Z",
     "shell.execute_reply": "2025-11-15T14:00:24.010435Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple session/memory helpers\n",
    "def save_session(session_id: str, data: Dict):\n",
    "    path = os.path.join(SESSION_DIR, f'{session_id}.json')\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    logger.info('Saved session %s', session_id)\n",
    "\n",
    "def load_session(session_id: str) -> Dict:\n",
    "    path = os.path.join(SESSION_DIR, f'{session_id}.json')\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Context compaction: keep only top-N most relevant abstracts\n",
    "def compact_context(title: str, papers: List[Dict], max_chars: int = 4000) -> str:\n",
    "    title_tokens = set(t.lower() for t in title.split())\n",
    "    def score(p):\n",
    "        text = (p.get('title','') + ' ' + p.get('abstract','')).lower()\n",
    "        return sum(1 for tok in title_tokens if tok in text)\n",
    "    papers_sorted = sorted(papers, key=score, reverse=True)\n",
    "    out = ''\n",
    "    for p in papers_sorted:\n",
    "        chunk = f\"Title: {p.get('title')}\\nAuthors: {p.get('authors')}\\nAbstract: {p.get('abstract')}\\nURL: {p.get('url')}\\n\\n\"\n",
    "        if len(out) + len(chunk) > max_chars:\n",
    "            break\n",
    "        out += chunk\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4373a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:00:24.014825Z",
     "iopub.status.busy": "2025-11-15T14:00:24.014585Z",
     "iopub.status.idle": "2025-11-15T14:00:24.020938Z",
     "shell.execute_reply": "2025-11-15T14:00:24.018882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agent dataclasses and base classes\n",
    "@dataclass\n",
    "class Paper:\n",
    "    title: str\n",
    "    authors: str\n",
    "    abstract: str\n",
    "    url: str\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "    def run(self, *args, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00fa3835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:00:24.023249Z",
     "iopub.status.busy": "2025-11-15T14:00:24.023019Z",
     "iopub.status.idle": "2025-11-15T14:00:24.028986Z",
     "shell.execute_reply": "2025-11-15T14:00:24.028064Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResearchAgent(BaseAgent):\n",
    "    \"\"\"Searches arXiv and returns a list of papers (title, authors, abstract, url).\"\"\"\n",
    "    def __init__(self, max_results=5):\n",
    "        super().__init__('ResearchAgent')\n",
    "        self.max_results = max_results\n",
    "\n",
    "    def run(self, query: str) -> List[Dict]:\n",
    "        logger.info('ResearchAgent searching arXiv for query: %s', query)\n",
    "        try:\n",
    "            search = arxiv.Search(query=query, max_results=self.max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
    "            results = []\n",
    "            for r in search.results():\n",
    "                results.append({\n",
    "                    'title': r.title,\n",
    "                    'authors': ', '.join([a.name for a in r.authors]),\n",
    "                    'abstract': (r.summary or '').replace('\\n',' '),\n",
    "                    'url': r.entry_id\n",
    "                })\n",
    "            logger.info('ResearchAgent found %d papers', len(results))\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.exception('ResearchAgent failed: %s', e)\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18067a1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:00:24.032299Z",
     "iopub.status.busy": "2025-11-15T14:00:24.031856Z",
     "iopub.status.idle": "2025-11-15T14:00:24.038149Z",
     "shell.execute_reply": "2025-11-15T14:00:24.037292Z"
    }
   },
   "outputs": [],
   "source": [
    "class WriterAgent(BaseAgent):\n",
    "    \"\"\"Generates a draft paper. Uses Gemini/GenAI if available, otherwise a template-based writer.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__('WriterAgent')\n",
    "\n",
    "    def run(self, title: str, context: str) -> str:\n",
    "        logger.info('WriterAgent generating draft for: %s', title)\n",
    "        prompt = f\"Write a detailed academic paper in IMRaD format for the title: '{title}'. Include Abstract, Introduction, Methods, Results, Discussion, Conclusion, and References. Use the following research context:\\n{context}\"\n",
    "        if _HAS_GENAI:\n",
    "            try:\n",
    "                resp = genai.GenerativeModel('gemini-pro').generate_content(prompt)\n",
    "                return resp.text\n",
    "            except Exception as e:\n",
    "                logger.warning('GenAI write failed, falling back to template: %s', e)\n",
    "\n",
    "        abstract = context[:800].strip() or ('This paper discusses ' + title)\n",
    "        intro = f\"Introduction:\\nThis paper addresses {title}. Context and related work: {context[:1500]}\"\n",
    "        methods = 'Methods:\\nThis is a simulated demo. Methods would include literature review and synthesis.'\n",
    "        results = 'Results:\\nThis notebook produces a synthetic paper draft based on retrieved abstracts.'\n",
    "        discussion = 'Discussion:\\nInterpretation of results and limitations.'\n",
    "        conclusion = 'Conclusion:\\nSummary and future work.'\n",
    "        references = 'References:\\n(See arXiv links in context.)'\n",
    "        paper = f\"Abstract:\\n{abstract}\\n\\n{intro}\\n\\n{methods}\\n\\n{results}\\n\\n{discussion}\\n\\n{conclusion}\\n\\n{references}\"\n",
    "        return paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90976a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:00:24.041061Z",
     "iopub.status.busy": "2025-11-15T14:00:24.040372Z",
     "iopub.status.idle": "2025-11-15T14:00:24.045985Z",
     "shell.execute_reply": "2025-11-15T14:00:24.044906Z"
    }
   },
   "outputs": [],
   "source": [
    "class EditorAgent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        super().__init__('EditorAgent')\n",
    "\n",
    "    def run(self, paper_text: str) -> str:\n",
    "        logger.info('EditorAgent editing paper (lightweight edits)')\n",
    "        text = paper_text.strip()\n",
    "        for h in ['Abstract:', 'Introduction:', 'Methods:', 'Results:', 'Discussion:', 'Conclusion:', 'References:']:\n",
    "            if h not in text:\n",
    "                text = h + '\\n' + text\n",
    "        lines = [l.rstrip() for l in text.splitlines()]\n",
    "        cleaned = []\n",
    "        prev_blank = False\n",
    "        for l in lines:\n",
    "            if not l:\n",
    "                if not prev_blank:\n",
    "                    cleaned.append('')\n",
    "                prev_blank = True\n",
    "            else:\n",
    "                cleaned.append(l)\n",
    "                prev_blank = False\n",
    "        return '\\n'.join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba540336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:00:24.049544Z",
     "iopub.status.busy": "2025-11-15T14:00:24.049240Z",
     "iopub.status.idle": "2025-11-15T14:00:24.056060Z",
     "shell.execute_reply": "2025-11-15T14:00:24.054495Z"
    }
   },
   "outputs": [],
   "source": [
    "class PlagiarismAgent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        super().__init__('PlagiarismAgent')\n",
    "\n",
    "    def run(self, paper_text: str, source_abstracts: List[str]) -> Dict:\n",
    "        \"\"\"Compute simple similarity scores between the paper text and known abstracts.\"\"\"\n",
    "        logger.info('PlagiarismAgent comparing paper against %d source abstracts', len(source_abstracts))\n",
    "        scores = []\n",
    "        for a in source_abstracts:\n",
    "            try:\n",
    "                s = difflib.SequenceMatcher(None, paper_text, a).ratio()\n",
    "            except Exception:\n",
    "                s = 0.0\n",
    "            scores.append(s)\n",
    "        max_score = max(scores) if scores else 0.0\n",
    "        avg_score = sum(scores)/len(scores) if scores else 0.0\n",
    "        report = {\n",
    "            'max_similarity': round(max_score*100,2),\n",
    "            'avg_similarity': round(avg_score*100,2),\n",
    "            'flags': ['High similarity with source abstract'] if max_score > 0.6 else []\n",
    "        }\n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5503525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:00:24.058338Z",
     "iopub.status.busy": "2025-11-15T14:00:24.058119Z",
     "iopub.status.idle": "2025-11-15T14:00:24.065302Z",
     "shell.execute_reply": "2025-11-15T14:00:24.064159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Orchestration workflow\n",
    "def research_agent_workflow(title: str, session_id: str = None, max_results: int = 5):\n",
    "    start_time = time.time()\n",
    "    session_id = session_id or title.replace(' ','_')[:40]\n",
    "    session = load_session(session_id) or {'title': title, 'steps': []}\n",
    "\n",
    "    research_agent = ResearchAgent(max_results=max_results)\n",
    "    t0 = time.time()\n",
    "    papers = research_agent.run(title)\n",
    "    dt = time.time() - t0\n",
    "    session['steps'].append({'step': 'research', 'count': len(papers), 'time': dt})\n",
    "\n",
    "    context = compact_context(title, papers)\n",
    "\n",
    "    writer = WriterAgent()\n",
    "    t0 = time.time()\n",
    "    draft = writer.run(title, context)\n",
    "    dt = time.time() - t0\n",
    "    session['steps'].append({'step': 'draft', 'time': dt})\n",
    "\n",
    "    editor = EditorAgent()\n",
    "    t0 = time.time()\n",
    "    edited = editor.run(draft)\n",
    "    dt = time.time() - t0\n",
    "    session['steps'].append({'step': 'edit', 'time': dt})\n",
    "\n",
    "    plagiarism = PlagiarismAgent()\n",
    "    t0 = time.time()\n",
    "    abstracts = [p['abstract'] for p in papers]\n",
    "    report = plagiarism.run(edited, abstracts)\n",
    "    dt = time.time() - t0\n",
    "    session['steps'].append({'step': 'plagiarism', 'time': dt})\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    session['metrics'] = {'total_time': total_time, 'num_papers': len(papers)}\n",
    "    session['papers'] = papers\n",
    "    session['draft'] = draft[:2000]\n",
    "    session['edited'] = edited[:2000]\n",
    "    session['plagiarism'] = report\n",
    "    save_session(session_id, session)\n",
    "    logger.info('Workflow complete: total_time=%.2fs, num_papers=%d', total_time, len(papers))\n",
    "    return {'session_id': session_id, 'session': session, 'draft': draft, 'edited': edited, 'plagiarism': report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e2d9ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:00:24.068493Z",
     "iopub.status.busy": "2025-11-15T14:00:24.068095Z",
     "iopub.status.idle": "2025-11-15T14:00:46.207977Z",
     "shell.execute_reply": "2025-11-15T14:00:46.206006Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 19:30:24,071 INFO ResearchAgent searching arXiv for query: The Role of AI Agents in Enhancing Academic Writing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kupen\\AppData\\Local\\Temp\\ipykernel_6608\\414929393.py:12: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for r in search.results():\n",
      "2025-11-15 19:30:24,072 INFO Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=The+Role+of+AI+Agents+in+Enhancing+Academic+Writing&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 19:30:33,960 INFO Got first page: 100 of 413955 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 19:30:33,968 INFO ResearchAgent found 5 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 19:30:33,971 INFO WriterAgent generating draft for: The Role of AI Agents in Enhancing Academic Writing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 19:30:46,176 WARNING GenAI write failed, falling back to template: \n",
      "  No API_KEY or ADC found. Please either:\n",
      "    - Set the `GOOGLE_API_KEY` environment variable.\n",
      "    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n",
      "    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 19:30:46,178 INFO EditorAgent editing paper (lightweight edits)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 19:30:46,180 INFO PlagiarismAgent comparing paper against 5 source abstracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 19:30:46,201 INFO Saved session example_session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 19:30:46,203 INFO Workflow complete: total_time=22.13s, num_papers=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: example_session\n",
      "Plagiarism report: {'max_similarity': 64.04, 'avg_similarity': 14.77, 'flags': ['High similarity with source abstract']}\n"
     ]
    }
   ],
   "source": [
    "# Example runner\n",
    "if __name__ == '__main__':\n",
    "    example_title = 'The Role of AI Agents in Enhancing Academic Writing'\n",
    "    out = research_agent_workflow(example_title, session_id='example_session', max_results=5)\n",
    "    print('Session ID:', out['session_id'])\n",
    "    print('Plagiarism report:', out['plagiarism'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
